# -*- coding: utf-8 -*-
"""AudioSTT_Translator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OYz2Nry9kEZqOXl3wbYk-o995v-aD-7t

# Installation
"""

!pip install git+https://github.com/openai/whisper.git

!sudo apt update && sudo apt install ffmpeg

!pip install pydub

!pip install -U deep-translator
!pip install -U deep-translator[ai]

!pip install yt-dlp

"""# Using your OpenAI API Key to Access GPT-3 (Only use if you want to use GPT-3) | Optional
You can get your API Key by clicking on Personal -> View API Keys

Or visit this website: https://platform.openai.com/account/api-keys

If you don't want to use GPT, Google Translate will be utilized, just proceed without this step.
"""

import openai

openai.api_key = "" #@param {type: "string"}
key = openai.api_key

"""# For Files


Note: Drag audio file to the right. Specific folder is at /content/
"""

audiofile_name = "" #@param {type: "string"}
audiofile_type = "wav" #@param ["mp3", "wav"]

audio = audiofile_name  + "." + audiofile_type
audio

"""#For Youtube Videos"""

import yt_dlp
from yt_dlp import YoutubeDL

URL = '' #@param {type: "string"}
format = 'mp3' #@param ['mp3', 'wav']

ydl_opts = {
    'format': '{format}/bestaudio/best',
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': format,
    }]
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URL)

filename = !yt-dlp --print filename {URL}

def listToString(s):
    # initialize an empty string
    filestr = ""
    # traverse in the string
    for ele in s:
        filestr += ele
    # return string
    return filestr
 
setaudio = listToString(filename)
audio = setaudio.replace('.webm', '.' + str(format))
audio

"""# Specify the model to be used


"""

#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |
#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |
#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |
#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |

ml = "small" #@param ["tiny", "base", "small", "medium", "large"]

"""# Transcription"""

import whisper

model = whisper.load_model(ml)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio(audio)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f"Detected language: {max(probs, key=probs.get)}")

lang = max(probs, key=probs.get)

result = model.transcribe(audio)
print(result["text"])

transcript = result["text"]
transcript

"""# Translating | Google Translate"""

from deep_translator import GoogleTranslator

#If you want to check supported languages
langs_dict = GoogleTranslator().get_supported_languages(as_dict=True)
langs_dict

#Source Language of the File
# Use 'lang' if you want to automatically detect.
src_lang = lang #@param {type: "string"}

#Target Language to translate to
tgt_lang = "ja" #@param {type: "string"}

translated = GoogleTranslator(source=src_lang, target=tgt_lang).translate(text=transcript)
translated

"""### Translating | GPT"""

from deep_translator import ChatGptTranslator
translated = ChatGptTranslator(api_key = key, target = tgt_lang).translate(text=transcript)
translated